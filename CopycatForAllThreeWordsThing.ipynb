{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programminghistorian.org/en/lessons/common-similarity-measures\n",
    "##should be the reference for the all process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "Analyzing Documents with TF-IDF\n",
    "主要衡量词频和某类型词语出现次数与分布程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programminghistorian.org/en/lessons/clustering-visualizing-word-embeddings\n",
    "Clustering and Visualising Documents using Word Embeddings\n",
    "词语嵌入（这个也许有用\n",
    "https://github.com/programminghistorian/jekyll/blob/gh-pages/assets/clustering-visualizing-word-embeddings/clustering-visualizing-word-embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed for the dimensionality reduction stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Module not found, will try to install...\")\n",
    "    !pip install umap-learn\n",
    "    import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed for hierarchical clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from kneed import KneeLocator\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Module not found, will try to install...\")\n",
    "    !pip install kneed\n",
    "    from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, centroid\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed for the validation and visualisation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url      = 'https://github.com/MaartenGr/cTFIDF/archive/refs/tags/v0.1.1.tar.gz'\n",
    "version  = re.search(r'/v(.+?)\\.tar\\.gz', url).group(1)\n",
    "dir_name = f'cTFIDF-{version}'\n",
    "dir_path = os.path.join(os.getcwd(),dir_name)\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "\n",
    "    print(\"Module not found, will try to download and prepare...\")\n",
    "\n",
    "    import requests, tarfile\n",
    "\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(f'{dir_name}.tar.gz', 'wb').write(r.content)\n",
    "    print(\"  Downloaded\")\n",
    "\n",
    "    tarf = tarfile.open(f'{dir_name}.tar.gz', 'r')\n",
    "    for f in tarf.getnames():\n",
    "        if not (f.startswith('/') or f.startswith('.')):\n",
    "            tarf.extract(f)\n",
    "    tarf.close()\n",
    "    os.remove(f'{dir_name}.tar.gz')\n",
    "\n",
    "    print(f\"Downloaded and unpacked cTFIDF-{version}.\")\n",
    "\n",
    "import sys\n",
    "if sys.path[-1] != dir_path:\n",
    "    sys.path.append(dir_path)\n",
    "\n",
    "try:\n",
    "    from ctfidf import CTFIDFVectorizer\n",
    "    print(\"Loaded Class-TF/IDF Vectorizer.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Still can't load Class-TF/IDF Vectorizer.\")\n",
    "\n",
    "    print(\"=\"*25)\n",
    "    print(\"You should try restarting the kernel now.\\nFor some reason unpacking and loading\\nimmediately doesn't work.\")\n",
    "    print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded Class-TF/IDF Vectorizer.\n",
    "\n",
    " Stop: if you still have errors after running the above code block for the first time then you will probably have to Restart the Kernel at this point. This code is trying to download a new module for which no installer exists and then register it with Python, but the process doesn't seem bullet-proof in my testing. Sorry, but you should only need to restart the Kernel this first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tries to find a narrow sans-serif TTF font by path that is slightly nicer than the default for the WordCloud library. You would need to update this default for your own system. You can list available fonts using (/ht imsc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "print(\"Possible fonts: \", end=\"\")\n",
    "for f in sorted(fonts):\n",
    "    if 'Narrow' in f:\n",
    "        print(f.split(os.path.sep)[-1], end=\", \")\n",
    "    elif 'Sans' in f:\n",
    "        print(f.split(os.path.sep)[-1], end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = fonts[0] # Ensure at least _something_ is set here\n",
    "for f in fonts:\n",
    "    if 'LiberationSansNarrow-Regular' in f:\n",
    "        fp = f.split(os.path.sep)[-1].split('.')[0]\n",
    "        break\n",
    "    elif 'Arial Narrow.ttf' in f:\n",
    "        fp = f.split(os.path.sep)[-1].split('.')[0]\n",
    "        break\n",
    "    elif 'Narrow' in f:\n",
    "        fp = f.split(os.path.sep)[-1].split('.')[0]\n",
    "print(f\"Using font: {fp}\")\n",
    "\n",
    "fname = ''.join([f' {x}' if x==x.upper() else x for x in fp.split('-')[0]]).strip().replace('  ','')\n",
    "print(f\"  Guessing at font name: {fname}\")\n",
    "\n",
    "# These are font dictionaries for the 's'uper-title, 't'itle,\n",
    "# 'a'xis, and 'l'abels.\n",
    "sfont = {'fontname':fname, 'fontsize':16}\n",
    "tfont = {'fontname':fname, 'fontsize':12}\n",
    "afont = {'fontname':fname, 'fontsize':10}\n",
    "lfont = {'fontname':fname, 'fontsize':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "rs = 43\n",
    "\n",
    "# Which embeddings to use\n",
    "src_embeddings = 'doc_vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data\n",
    "In this tutorial I make use of the modern Parquet format: it's highly-compressed and columnar, so it works very well (and quickly) with large data sets. The file can also be read directly in DuckDB if you use it, but the general idea is to minimise the volume of data transfered. The columnar orientation means that you can quickly load only the columns that you need for an analysis, and don't have to read in the entire data set each time (as you would with, say, CSV or most other common data formats)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
